{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this file:\n",
    "\n",
    "This notebook is just an implementation of LLMs Head Importance Calculation method. This method is described in the following paper: https://www.arxiv.org/pdf/2407.14679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from typing import List\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMHeadImportance:\n",
    "    def __init__(seld, model_name: str = \"bert-base-uncased\"):\n",
    "        seld.model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "        seld.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        seld.model.eval()\n",
    "    \n",
    "    def compute_head_importance(self, text: str) -> List[float]:\n",
    "\n",
    "        # Tokenize the input fox\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "        # Get attention weights\n",
    "        attentions = outputs.attentions # This is a tuple of attention weights for each layer\n",
    "\n",
    "        head_importance = []\n",
    "        for layer_attentions in attentions:\n",
    "\n",
    "            # Layer_attentions shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
    "            layer_importance = []\n",
    "\n",
    "            for head_idx in range(layer_attentions.size(1)):\n",
    "\n",
    "                head_attention = layer_attentions[0, head_idx, :, :]\n",
    "\n",
    "                # Compute the frobenius norm of the attention matrix\n",
    "                importance = torch.norm(head_attention, p='fro')\n",
    "                layer_importance.append(importance.item())\n",
    "            \n",
    "            head_importance.append(layer_importance)\n",
    "\n",
    "        return head_importance\n",
    "    \n",
    "    def aggregate_importance(self, scores: List[float], method: str = \"mean\") -> float:\n",
    "        if method == \"mean\":\n",
    "            return sum(scores) / len(scores)\n",
    "        elif method == \"l2_norm\":\n",
    "            return (sum([s**2 for s in scores])**0.5)\n",
    "        elif method == \"variance\":\n",
    "            mean = sum(scores) / len(scores) \n",
    "            return sum([(s - mean)**2 for s in scores]) / len(scores)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation method: {method}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vishal\\Github\\PyTorch-Grind\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Importance Scores: \n",
      "Layer 0, Head 0: 8.1735\n",
      "Layer 0, Head 1: 8.8312\n",
      "Layer 0, Head 2: 21.6100\n",
      "Layer 0, Head 3: 21.7919\n",
      "Layer 0, Head 4: 16.5113\n",
      "Layer 0, Head 5: 12.6093\n",
      "Layer 0, Head 6: 16.9935\n",
      "Layer 0, Head 7: 8.6542\n",
      "Layer 0, Head 8: 11.5274\n",
      "Layer 0, Head 9: 11.0889\n",
      "Layer 0, Head 10: 22.2144\n",
      "Layer 0, Head 11: 18.9474\n",
      "Layer 1, Head 0: 19.1049\n",
      "Layer 1, Head 1: 13.4629\n",
      "Layer 1, Head 2: 11.1345\n",
      "Layer 1, Head 3: 15.2564\n",
      "Layer 1, Head 4: 13.5162\n",
      "Layer 1, Head 5: 11.4204\n",
      "Layer 1, Head 6: 22.2157\n",
      "Layer 1, Head 7: 12.3090\n",
      "Layer 1, Head 8: 10.7550\n",
      "Layer 1, Head 9: 7.6027\n",
      "Layer 1, Head 10: 15.6025\n",
      "Layer 1, Head 11: 7.3570\n",
      "Layer 2, Head 0: 19.0868\n",
      "Layer 2, Head 1: 18.1804\n",
      "Layer 2, Head 2: 16.5020\n",
      "Layer 2, Head 3: 10.1512\n",
      "Layer 2, Head 4: 15.5794\n",
      "Layer 2, Head 5: 15.0479\n",
      "Layer 2, Head 6: 10.2510\n",
      "Layer 2, Head 7: 14.1339\n",
      "Layer 2, Head 8: 11.7780\n",
      "Layer 2, Head 9: 19.9995\n",
      "Layer 2, Head 10: 9.6312\n",
      "Layer 2, Head 11: 15.1023\n",
      "Layer 3, Head 0: 15.7930\n",
      "Layer 3, Head 1: 12.4936\n",
      "Layer 3, Head 2: 12.7143\n",
      "Layer 3, Head 3: 15.4348\n",
      "Layer 3, Head 4: 8.2682\n",
      "Layer 3, Head 5: 20.3966\n",
      "Layer 3, Head 6: 13.5894\n",
      "Layer 3, Head 7: 11.3967\n",
      "Layer 3, Head 8: 11.0660\n",
      "Layer 3, Head 9: 14.0479\n",
      "Layer 3, Head 10: 14.4012\n",
      "Layer 3, Head 11: 15.5895\n",
      "Layer 4, Head 0: 10.9851\n",
      "Layer 4, Head 1: 8.3057\n",
      "Layer 4, Head 2: 12.1807\n",
      "Layer 4, Head 3: 10.9034\n",
      "Layer 4, Head 4: 8.0998\n",
      "Layer 4, Head 5: 18.6281\n",
      "Layer 4, Head 6: 16.0539\n",
      "Layer 4, Head 7: 16.9138\n",
      "Layer 4, Head 8: 18.0295\n",
      "Layer 4, Head 9: 14.7794\n",
      "Layer 4, Head 10: 14.9986\n",
      "Layer 4, Head 11: 13.5101\n",
      "Layer 5, Head 0: 19.0013\n",
      "Layer 5, Head 1: 18.6194\n",
      "Layer 5, Head 2: 11.5145\n",
      "Layer 5, Head 3: 17.7626\n",
      "Layer 5, Head 4: 15.7909\n",
      "Layer 5, Head 5: 20.3484\n",
      "Layer 5, Head 6: 20.1824\n",
      "Layer 5, Head 7: 21.5548\n",
      "Layer 5, Head 8: 20.8225\n",
      "Layer 5, Head 9: 19.3160\n",
      "Layer 5, Head 10: 15.8194\n",
      "Layer 5, Head 11: 20.4197\n",
      "Layer 6, Head 0: 16.1567\n",
      "Layer 6, Head 1: 13.4233\n",
      "Layer 6, Head 2: 17.1669\n",
      "Layer 6, Head 3: 18.6445\n",
      "Layer 6, Head 4: 18.9717\n",
      "Layer 6, Head 5: 19.9799\n",
      "Layer 6, Head 6: 15.6384\n",
      "Layer 6, Head 7: 12.3956\n",
      "Layer 6, Head 8: 13.5578\n",
      "Layer 6, Head 9: 16.3589\n",
      "Layer 6, Head 10: 18.5786\n",
      "Layer 6, Head 11: 15.9249\n",
      "Layer 7, Head 0: 15.8054\n",
      "Layer 7, Head 1: 18.4439\n",
      "Layer 7, Head 2: 15.3362\n",
      "Layer 7, Head 3: 18.4927\n",
      "Layer 7, Head 4: 14.4789\n",
      "Layer 7, Head 5: 16.2160\n",
      "Layer 7, Head 6: 20.1343\n",
      "Layer 7, Head 7: 14.8787\n",
      "Layer 7, Head 8: 19.7951\n",
      "Layer 7, Head 9: 15.6562\n",
      "Layer 7, Head 10: 18.1870\n",
      "Layer 7, Head 11: 15.7266\n",
      "Layer 8, Head 0: 18.1810\n",
      "Layer 8, Head 1: 15.2274\n",
      "Layer 8, Head 2: 12.5231\n",
      "Layer 8, Head 3: 17.0790\n",
      "Layer 8, Head 4: 12.2746\n",
      "Layer 8, Head 5: 14.1668\n",
      "Layer 8, Head 6: 14.5275\n",
      "Layer 8, Head 7: 16.9994\n",
      "Layer 8, Head 8: 14.3632\n",
      "Layer 8, Head 9: 17.0740\n",
      "Layer 8, Head 10: 14.6786\n",
      "Layer 8, Head 11: 17.2709\n",
      "Layer 9, Head 0: 12.4657\n",
      "Layer 9, Head 1: 13.7410\n",
      "Layer 9, Head 2: 12.4563\n",
      "Layer 9, Head 3: 12.2768\n",
      "Layer 9, Head 4: 14.1767\n",
      "Layer 9, Head 5: 11.9037\n",
      "Layer 9, Head 6: 12.0039\n",
      "Layer 9, Head 7: 16.6203\n",
      "Layer 9, Head 8: 14.2731\n",
      "Layer 9, Head 9: 12.5015\n",
      "Layer 9, Head 10: 15.6495\n",
      "Layer 9, Head 11: 15.6885\n",
      "Layer 10, Head 0: 14.9353\n",
      "Layer 10, Head 1: 15.1542\n",
      "Layer 10, Head 2: 13.9532\n",
      "Layer 10, Head 3: 16.2919\n",
      "Layer 10, Head 4: 11.6575\n",
      "Layer 10, Head 5: 12.0106\n",
      "Layer 10, Head 6: 13.8468\n",
      "Layer 10, Head 7: 12.3595\n",
      "Layer 10, Head 8: 19.8761\n",
      "Layer 10, Head 9: 15.8881\n",
      "Layer 10, Head 10: 13.5666\n",
      "Layer 10, Head 11: 11.0961\n",
      "Layer 11, Head 0: 12.7832\n",
      "Layer 11, Head 1: 10.5110\n",
      "Layer 11, Head 2: 16.4495\n",
      "Layer 11, Head 3: 11.3724\n",
      "Layer 11, Head 4: 13.6619\n",
      "Layer 11, Head 5: 11.2344\n",
      "Layer 11, Head 6: 7.9677\n",
      "Layer 11, Head 7: 11.1811\n",
      "Layer 11, Head 8: 14.3425\n",
      "Layer 11, Head 9: 13.8997\n",
      "Layer 11, Head 10: 13.6745\n",
      "Layer 11, Head 11: 10.0405\n",
      "Mean: 177.4742\n",
      "L2 Norm: 182.1946\n",
      "Variance: 317737.1564\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # create an importance caluculator\n",
    "    importance_calculator = LLMHeadImportance(\"bert-base-uncased\")\n",
    "\n",
    "    # input text\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "    # compute the importance of each head\n",
    "    head_importance = importance_calculator.compute_head_importance(text)\n",
    "\n",
    "    print(\"Head Importance Scores: \")\n",
    "    for i, layer_importance in enumerate(head_importance):\n",
    "        for j, score in enumerate(layer_importance):\n",
    "            print(f\"Layer {i}, Head {j}: {score:.4f}\")\n",
    "\n",
    "    # aggregate the importance scores\n",
    "    print(f\"Mean: {sum(itertools.chain(*head_importance)) / len(head_importance):.4f}\")\n",
    "    print(f\"L2 Norm: {(sum([s**2 for s in itertools.chain(*head_importance)])**(0.5)):.4f}\")\n",
    "    print(f\"Variance: {(sum([(s - sum(itertools.chain(*head_importance)) / len(head_importance))**2 for s in itertools.chain(*head_importance)]) / len(head_importance)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
