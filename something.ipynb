{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vishal\\Github\\PyTorch-Grind\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "def load_calibration_data(tokenizer, max_length=512, batch_size=32):\n",
    "    # Load a small subset of C4 dataset\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"train[:1%]\")\n",
    "    texts = dataset['text']\n",
    "    dataset = TextDataset(texts, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def load_train_data(tokenizer, max_length=512, batch_size=32):\n",
    "    # Load the full C4 dataset\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"train\")\n",
    "    texts = dataset['text']\n",
    "    dataset = TextDataset(texts, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def load_val_data(tokenizer, max_length=512, batch_size=32):\n",
    "    # Load the validation split of C4 dataset\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"validation\")\n",
    "    texts = dataset['text']\n",
    "    dataset = TextDataset(texts, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vishal\\Github\\PyTorch-Grind\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_calibration_data() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m     train(teacher_model, student_model, train_data, val_data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 136\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m student_model\u001b[38;5;241m.\u001b[39mload_state_dict(teacher_model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Load calibration data\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m calibration_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_calibration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Estimate importance\u001b[39;00m\n\u001b[0;32m    139\u001b[0m importances \u001b[38;5;241m=\u001b[39m estimate_importance(teacher_model, calibration_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: load_calibration_data() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple transformer model (you'd use a full LLM implementation in practice)\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Importance estimation function for width pruning\n",
    "def estimate_importance(model, calibration_data):\n",
    "    importances = {\n",
    "        'heads': [],\n",
    "        'neurons': [],\n",
    "        'emb_channels': []\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in calibration_data:\n",
    "            # Compute importances based on activations\n",
    "            # This is a simplified version - you'd need to implement the full logic for each layer\n",
    "            for layer in model.transformer.layers:\n",
    "                # Compute head importance\n",
    "                q, k, v = layer.self_attn.in_proj_weight.chunk(3, dim=0)\n",
    "                qkv = torch.cat([q.unsqueeze(0), k.unsqueeze(0), v.unsqueeze(0)], dim=0)\n",
    "                head_importance = torch.norm(qkv, p=2, dim=-1).sum(dim=-1)\n",
    "                importances['heads'].append(head_importance)\n",
    "                \n",
    "                # Compute neuron importance\n",
    "                neuron_importance = torch.norm(layer.linear1.weight, p=2, dim=1)\n",
    "                importances['neurons'].append(neuron_importance)\n",
    "            \n",
    "            # Compute embedding channel importance\n",
    "            emb_importance = torch.norm(model.embedding.weight, p=2, dim=0)\n",
    "            importances['emb_channels'].append(emb_importance)\n",
    "\n",
    "    # Aggregate importances\n",
    "    for key in importances:\n",
    "        importances[key] = torch.stack(importances[key]).sum(dim=0)\n",
    "\n",
    "    return importances\n",
    "\n",
    "# Pruning function\n",
    "def prune_model(model, importances, prune_ratio):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            if 'self_attn.in_proj_weight' in name:\n",
    "                # Prune attention heads\n",
    "                num_heads = model.transformer.layers[0].self_attn.num_heads\n",
    "                head_importance = importances['heads']\n",
    "                k = int(num_heads * (1 - prune_ratio))\n",
    "                top_heads = torch.topk(head_importance, k).indices\n",
    "                mask = torch.zeros_like(param)\n",
    "                for head in top_heads:\n",
    "                    start = head * param.size(0) // num_heads\n",
    "                    end = (head + 1) * param.size(0) // num_heads\n",
    "                    mask[start:end] = 1\n",
    "                param.data *= mask\n",
    "            elif 'linear1.weight' in name:\n",
    "                # Prune neurons\n",
    "                neuron_importance = importances['neurons']\n",
    "                k = int(param.size(0) * (1 - prune_ratio))\n",
    "                top_neurons = torch.topk(neuron_importance, k).indices\n",
    "                mask = torch.zeros_like(param)\n",
    "                mask[top_neurons] = 1\n",
    "                param.data *= mask\n",
    "            elif 'embedding.weight' in name:\n",
    "                # Prune embedding channels\n",
    "                emb_importance = importances['emb_channels']\n",
    "                k = int(param.size(1) * (1 - prune_ratio))\n",
    "                top_channels = torch.topk(emb_importance, k).indices\n",
    "                mask = torch.zeros_like(param)\n",
    "                mask[:, top_channels] = 1\n",
    "                param.data *= mask\n",
    "\n",
    "# Knowledge distillation loss\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=1.0):\n",
    "    return nn.KLDivLoss(reduction='batchmean')(\n",
    "        F.log_softmax(student_logits / temperature, dim=-1),\n",
    "        F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    ) * (temperature ** 2)\n",
    "\n",
    "# Main training loop\n",
    "def train(teacher_model, student_model, train_data, val_data, epochs):\n",
    "    optimizer = optim.Adam(student_model.parameters())\n",
    "    teacher_model.eval()\n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        for batch in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            student_output = student_model(batch)\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(batch)\n",
    "            # Compute distillation loss\n",
    "            loss = distillation_loss(student_output, teacher_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation data\n",
    "        validate(student_model, val_data)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_data):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_data:\n",
    "            output = model(batch)\n",
    "            # Compute validation loss (e.g., cross-entropy)\n",
    "            loss = F.cross_entropy(output, batch['targets'])\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_loss / len(val_data)}\")\n",
    "\n",
    "# Main process\n",
    "def main():\n",
    "    # Initialize teacher model (pretrained LLM)\n",
    "    teacher_model = SimpleTransformer(vocab_size=30000, d_model=512, nhead=8, num_layers=6)\n",
    "    \n",
    "    # Create student model (copy of teacher)\n",
    "    student_model = SimpleTransformer(vocab_size=30000, d_model=512, nhead=8, num_layers=6)\n",
    "    student_model.load_state_dict(teacher_model.state_dict())\n",
    "    \n",
    "    # Load calibration data\n",
    "    calibration_data = load_calibration_data()\n",
    "    \n",
    "    # Estimate importance\n",
    "    importances = estimate_importance(teacher_model, calibration_data)\n",
    "    \n",
    "    # Prune student model\n",
    "    prune_model(student_model, importances, prune_ratio=0.5)\n",
    "    \n",
    "    # Load training and validation data\n",
    "    train_data = load_train_data()\n",
    "    val_data = load_val_data()\n",
    "    \n",
    "    # Train student model\n",
    "    train(teacher_model, student_model, train_data, val_data, epochs=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
